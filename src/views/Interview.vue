<template>
  <div class="container">
    <!-- ========= é¡¶éƒ¨å¯¼èˆª ========= -->
    <div class="top-nav">
      <a href="#" class="back-button">
        <!-- è¿”å›ç®­å¤´ -->
        <svg
          width="16"
          height="16"
          viewBox="0 0 24 24"
          fill="none"
          xmlns="http://www.w3.org/2000/svg"
        >
          <path
            d="M19 12H5M5 12L12 19M5 12L12 5"
            stroke="#4CAF50"
            stroke-width="2"
            stroke-linecap="round"
            stroke-linejoin="round"
          />
        </svg>
        è¿”å›ä¸»é¡µ
      </a>
      <div class="user-profile">
        <span class="user-name">{{ userName }}</span>
        <div class="user-avatar"></div>
      </div>
    </div>

    <!-- ========= ä¸»ä½“åŒºåŸŸ ========= -->
    <div class="main-content">
      <!-- ==== æ‘„åƒå¤´ä¸æŒ‡ç¤ºåŒº ==== -->
      <div class="camera-section">
        <div class="section-header">
          <p class="question-text">æ¨¡æ‹Ÿé¢˜ï¼š{{ question }}</p>
          <button class="refresh-btn" @click="showRandomQuestion">
            <span class="icon">ğŸ”„</span>
            æ¢ä¸€é¢˜
          </button>
        </div>

        <!-- æ‘„åƒå¤´ç”»é¢ -->
        <div class="camera-container">
          <!-- å·¦ä¸Šè§’ï¼šæ‘„åƒå¤´ / AI çŠ¶æ€ -->
          <div class="status-indicators">
            <div class="status-indicator">
              æ‘„åƒå¤´: {{ cameraOn ? "å·²å¼€å¯" : "ç­‰å¾…å¼€å¯" }}
            </div>
            <div class="analysis-indicator">
              AIåˆ†æ: {{ analysisReady ? "å·²å¼€å¯" : "å‡†å¤‡å°±ç»ª" }}
            </div>
          </div>

          <!-- è§†é¢‘ + ç»˜åˆ¶å±‚ -->
          <video ref="cameraView" autoplay playsinline></video>
          <canvas ref="overlay" class="overlay-canvas"></canvas>

          <!-- å·¦ä¸‹è§’ï¼šæ‰‹åŠ¿ / é¢éƒ¨å®æ—¶æŒ‡æ ‡ -->
          <div class="indicator-container">
            <div class="gesture-indicator">{{ gesture }}</div>
            <div class="face-indicator">{{ faceExpression }}</div>
          </div>
        </div>

        <!-- æ‘„åƒå¤´æ§åˆ¶æŒ‰é’® -->
        <div class="camera-controls">
          <button @click="startCamera" class="control-btn">å¼€å¯æ‘„åƒå¤´</button>
          <button
            @click="stopCamera"
            :disabled="!cameraOn"
            class="control-btn stop"
          >
            å…³é—­æ‘„åƒå¤´
          </button>
          <button @click="analyze" class="control-btn">
            {{ analysisReady ? "é‡æ–°åˆ†æ" : "å¼€å¯AIåˆ†æ" }}
          </button>
        </div>
      </div>

      <!-- ==== è¯­éŸ³ã€AI åé¦ˆåŒº ==== -->
      <div class="feedback-section">
        <div class="section-header">è¯­éŸ³è½¬æ–‡å­—</div>

        <div class="speech-panel">
          <div class="transcript">
            {{ transcript + interimTranscript }}
          </div>
          <div class="btn-group">
            <button class="control-btn stop" @click="startRecognition">
              {{ isRecognizing ? "åœæ­¢è¯†åˆ«" : "å¼€å§‹è¯†åˆ«" }}
            </button>
          </div>
        </div>

        <div class="section-header">
          AI é¢è¯•åé¦ˆ
          <span v-if="loadingEval" class="spinner"></span>
        </div>
        <div class="content">
          <div class="ai-feedback" id="feedback-container">
            <div
              v-for="(item, idx) in feedbackList"
              :key="idx"
              class="feedback-item"
              v-html="renderMarkdown(item)"
            ></div>
          </div>
        </div>
      </div>
    </div>

    <!-- ========= é¡µè„š ========= -->
    <div class="footer">
      <div class="footer-links">
        <a href="#" class="footer-link">å…³äºæˆ‘ä»¬</a>
        <a href="#" class="footer-link">éšç§æ”¿ç­–</a>
        <a href="#" class="footer-link">ä½¿ç”¨æ¡æ¬¾</a>
      </div>
      <div class="copyright">Â© 2025 AIé¢è¯•åŠ©æ‰‹ | ç”±æ™ºèƒ½äººæœºäº¤äº’å›¢é˜Ÿåˆ¶ä½œ</div>
      <div class="version">v1.0.0</div>
    </div>
  </div>
</template>

<script setup>
import { ref, onMounted, onUnmounted } from "vue";
import {
  GestureRecognizer,
  FaceLandmarker,
  FilesetResolver,
  DrawingUtils,
} from "@mediapipe/tasks-vision";
import OpenAI from "openai";
import { marked } from "marked";

const userName = ref("å¼ å°æ˜");
const cameraOn = ref(false);
const analysisReady = ref(false);
const feedbackList = ref([]);
const gesture = ref("æ— ");
const faceExpression = ref("é¢éƒ¨: æ— ");

const cameraView = ref(null);
const overlay = ref(null);
let overlayCtx = null;

let gestureRecognizer = null;
let faceLandmarker = null;
let drawingUtils = null;
let animationFrameId = null;

const transcript = ref("");
const interimTranscript = ref("");
const isRecognizing = ref(false);
let recognition = null;

const MEDIAPIPE_TASKS_VISION_VERSION = "0.10.22-rc.20250304";
const GESTURE_RECOGNIZER_TASK_URL = `https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task`;
const FACE_LANDMARKER_TASK_URL = `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`;
const VISION_WASM_URL_BASE = `https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@${MEDIAPIPE_TASKS_VISION_VERSION}/wasm`;

// æ–°å¢æŒ‡æ ‡ç›¸å…³å˜é‡
let frameCount = 0;
let blinkHistory = [];
let smileHistory = [];
let pressureHistory = [];
let emotionHistory = [];
let mouthHistory = [];
let relaxationHistory = [];
let microExpressionBuffer = [];

const MAX_HISTORY = 30;
const PRESSURE_HISTORY_LENGTH = 20;
const EMOTION_HISTORY_LENGTH = 30;
const MICRO_EXPRESSION_WINDOW = 15;

const interviewQuestions = [
  "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»ã€‚",
  "ä½ æœ€å¤§çš„ä¼˜ç‚¹å’Œç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
  "ä¸ºä»€ä¹ˆé€‰æ‹©åº”è˜æˆ‘ä»¬å…¬å¸ï¼Ÿ",
  "è°ˆè°ˆä½ æ›¾ç»é‡åˆ°çš„æœ€å›°éš¾çš„é¡¹ç›®åŠå¦‚ä½•è§£å†³ï¼Ÿ",
  "ä½ æœªæ¥äº”å¹´èŒä¸šè§„åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ",
  "å¦‚ä½•çœ‹å¾…å›¢é˜Ÿåˆä½œï¼Ÿ",
  "æè¿°ä¸€æ¬¡ä½ åœ¨å›¢é˜Ÿä¸­å‘æŒ¥é¢†å¯¼ä½œç”¨çš„ç»å†ã€‚",
  "ä½ å¦‚ä½•å¤„ç†å·¥ä½œä¸­çš„å‹åŠ›ï¼Ÿ",
  "ç»™æˆ‘è®²è®²ä½ åœ¨è¿‡å»å·¥ä½œä¸­æœ€éª„å‚²çš„æˆå°±ã€‚",
  "å¦‚æœä½ å’ŒåŒäº‹äº§ç”Ÿåˆ†æ­§ï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿ",
];

const question = ref("åŠ è½½ä¸­...");

function showRandomQuestion() {
  const idx = Math.floor(Math.random() * interviewQuestions.length);
  question.value = interviewQuestions[idx];
}

async function createGestureRecognizer() {
  console.log("[createGestureRecognizer] - Initializing...");
  try {
    const vision = await FilesetResolver.forVisionTasks(VISION_WASM_URL_BASE);
    gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
      baseOptions: {
        modelAssetPath: GESTURE_RECOGNIZER_TASK_URL,
        delegate: "GPU",
      },
      runningMode: "VIDEO",
      numHands: 1,
      minHandDetectionConfidence: 0.5,
      minHandPresenceConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });
    console.log("GestureRecognizer initialized successfully.");
  } catch (error) {
    console.error("Error initializing GestureRecognizer:", error);
    feedbackList.value = [`AIæ‰‹åŠ¿è¯†åˆ«æ¨¡å‹åŠ è½½å¤±è´¥: ${error.message}`];
    gestureRecognizer = null;
  }
}

async function createFaceLandmarker() {
  console.log("[createFaceLandmarker] - Initializing with blendshapes...");
  try {
    const vision = await FilesetResolver.forVisionTasks(VISION_WASM_URL_BASE);
    faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
      baseOptions: {
        modelAssetPath: FACE_LANDMARKER_TASK_URL,
        delegate: "GPU",
      },
      runningMode: "VIDEO",
      outputFaceBlendshapes: true,
      numFaces: 1,
      minFaceDetectionConfidence: 0.5,
      minFacePresenceConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });
    console.log("FaceLandmarker with blendshapes initialized successfully.");
  } catch (error) {
    console.error("Error initializing FaceLandmarker:", error);
    feedbackList.value = [`AIé¢éƒ¨è¯†åˆ«æ¨¡å‹åŠ è½½å¤±è´¥: ${error.message}`];
    faceLandmarker = null;
  }
}

async function startCamera() {
  console.log("[startCamera] - Attempting to start camera...");

  if (!gestureRecognizer) {
    await createGestureRecognizer();
  }
  if (!faceLandmarker) {
    await createFaceLandmarker();
  }

  if (!gestureRecognizer || !faceLandmarker) {
    feedbackList.value = ["AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¼€å¯æ‘„åƒå¤´ã€‚"];
    return;
  }

  if (cameraOn.value) {
    console.log("[startCamera] - Camera is already on.");
    return;
  }

  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    console.error(
      "[startCamera] - getUserMedia not supported on this browser."
    );
    feedbackList.value = ["æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒæ‘„åƒå¤´è®¿é—®ã€‚"];
    return;
  }

  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { width: { ideal: 1100 }, height: { ideal: 750 } },
    });

    if (cameraView.value) {
      cameraView.value.srcObject = stream;
      cameraView.value.onloadeddata = () => {
        console.log("Video data loaded. Starting prediction loop.");
        if (overlay.value) {
          overlayCtx = overlay.value.getContext("2d");
          if (overlayCtx) {
            drawingUtils = new DrawingUtils(overlayCtx);
            console.log("Canvas context and DrawingUtils initialized.");
            lastVideoTime = -1;
            cameraOn.value = true;
            if (cameraOn.value) predictWebcam();
          } else {
            console.error("Failed to get 2D context from overlay canvas.");
            feedbackList.value = ["æ— æ³•åˆå§‹åŒ–ç»˜å›¾åŒºåŸŸã€‚"];
            cameraOn.value = false;
            stream.getTracks().forEach((track) => track.stop());
          }
        } else {
          console.error("Overlay canvas element not found.");
          feedbackList.value = ["ç»˜å›¾åŒºåŸŸæœªæ‰¾åˆ°ã€‚"];
          cameraOn.value = false;
          stream.getTracks().forEach((track) => track.stop());
        }
      };
    } else {
      console.error("Video element (cameraView) not found.");
      stream.getTracks().forEach((track) => track.stop());
      cameraOn.value = false;
    }
  } catch (error) {
    console.error("[startCamera] - Error accessing webcam:", error);
    feedbackList.value = [`æ— æ³•è®¿é—®æ‘„åƒå¤´: ${error.message}`];
    cameraOn.value = false;
  }
}

let lastVideoTime = -1;
async function predictWebcam() {
  if (
    !cameraOn.value ||
    !cameraView.value ||
    !gestureRecognizer ||
    !faceLandmarker ||
    !drawingUtils ||
    !overlayCtx
  ) {
    if (animationFrameId) cancelAnimationFrame(animationFrameId);
    return;
  }

  if (
    cameraView.value.paused ||
    cameraView.value.ended ||
    cameraView.value.readyState < 2
  ) {
    animationFrameId = requestAnimationFrame(predictWebcam);
    return;
  }

  const video = cameraView.value;
  if (
    overlay.value &&
    (overlay.value.width !== video.videoWidth ||
      overlay.value.height !== video.videoHeight)
  ) {
    if (video.videoWidth > 0 && video.videoHeight > 0) {
      overlay.value.width = video.videoWidth;
      overlay.value.height = video.videoHeight;
    }
  }

  if (video.currentTime !== lastVideoTime) {
    lastVideoTime = video.currentTime;
    const startTimeMs = performance.now();

    overlayCtx.clearRect(0, 0, overlay.value.width, overlay.value.height);

    const gestureResults = gestureRecognizer.recognizeForVideo(
      video,
      startTimeMs
    );
    processGestureResults(gestureResults);

    const faceResults = faceLandmarker.detectForVideo(video, startTimeMs);
    processFaceResults(faceResults);
  }

  if (cameraOn.value) {
    animationFrameId = requestAnimationFrame(predictWebcam);
  }
}

function processGestureResults(results) {
  /*
  if (results.landmarks && results.landmarks.length > 0) {
    for (const landmarks of results.landmarks) {
      drawingUtils.drawConnectors(
        landmarks,
        GestureRecognizer.HAND_CONNECTIONS,
        {
          color: "#00FF00",
          lineWidth: 5,
        }
      );
      drawingUtils.drawLandmarks(landmarks, {
        color: "#FF0000",
        lineWidth: 2,
        radius: 3,
      });
    }
  }
  */

  if (
    results.gestures &&
    results.gestures.length > 0 &&
    results.gestures[0].length > 0
  ) {
    const topGesture = results.gestures[0][0];
    const categoryName = topGesture.categoryName;
    const categoryScore = parseFloat(topGesture.score * 100).toFixed(2);
    const handedness =
      results.handednesses &&
      results.handednesses.length > 0 &&
      results.handednesses[0].length > 0
        ? results.handednesses[0][0].displayName
        : "N/A";

    if (categoryName && categoryName !== "") {
      gesture.value = `${categoryName} (${categoryScore}%) - ${handedness}`;
    } else {
      gesture.value = `æœªçŸ¥æ‰‹åŠ¿ (${categoryScore}%) - ${handedness}`;
    }
  } else {
    gesture.value = "æ— æ‰‹åŠ¿";
  }
}

function extractExpressionFeatures(blendshapes) {
  return {
    eyeClosure: getBlendshapeScore(blendshapes, ["eyeBlink_L", "eyeBlink_R"]),
    eyeWideness:
      1 - getBlendshapeScore(blendshapes, ["eyeSquint_L", "eyeSquint_R"]),

    browFrown: getBlendshapeScore(blendshapes, ["browDown_L", "browDown_R"]),
    browRaise: getBlendshapeScore(blendshapes, ["browInnerUp"]),

    mouthSmile: getBlendshapeScore(blendshapes, [
      "mouthSmile_L",
      "mouthSmile_R",
    ]),
    mouthFrown: getBlendshapeScore(blendshapes, [
      "mouthFrown_L",
      "mouthFrown_R",
    ]),
    mouthOpen: getBlendshapeScore(blendshapes, ["mouthOpen", "jawOpen"]),
    mouthPress: getBlendshapeScore(blendshapes, [
      "mouthPress_L",
      "mouthPress_R",
    ]),

    jawTension: getBlendshapeScore(blendshapes, ["jawForward", "jawClench"]),
    noseWrinkle: getBlendshapeScore(blendshapes, [
      "noseSneer_L",
      "noseSneer_R",
    ]),

    cheekSquint: getBlendshapeScore(blendshapes, [
      "cheekSquint_L",
      "cheekSquint_R",
    ]),
  };
}

function analyzeEmotionalIndicators(blendshapes, frameCount) {
  const features = extractExpressionFeatures(blendshapes);

  return {
    eyeContact: calculateDynamicEyeContact(features, frameCount),
    smileNaturalness: calculateDynamicSmile(features, frameCount),
    focusLevel: calculateDynamicFocus(features, frameCount),
    articulation: calculateArticulation(features, frameCount),
    emotionalActivity: calculateEmotionalActivity(features, frameCount),
    emotionalPositivity: calculateEmotionalPositivity(features),
  };
}

function calculateDynamicEyeContact(features, frameCount) {
  const currentBlink = features.eyeClosure;

  blinkHistory.push(currentBlink);
  if (blinkHistory.length > MAX_HISTORY) blinkHistory.shift();

  const dynamicBaseline = 0.2 + 0.1 * Math.sin(frameCount / 100);
  const blinkFrequency =
    blinkHistory.filter((b) => b > 0.5).length / MAX_HISTORY;

  return Math.max(
    0,
    1 - (currentBlink * 0.7 + blinkFrequency * 0.3 + dynamicBaseline * 0.2)
  );
}

function calculateDynamicSmile(features, frameCount) {
  smileHistory.push(features.mouthSmile);
  if (smileHistory.length > 10) smileHistory.shift();

  const smileChangeRate =
    smileHistory.length > 1
      ? Math.abs(
          smileHistory[smileHistory.length - 1] -
            smileHistory[smileHistory.length - 2]
        )
      : 0;

  const naturalness =
    (features.mouthSmile * 0.4 +
      features.cheekSquint * 0.3 +
      features.eyeWideness * 0.3) *
    (1 - Math.min(1, smileChangeRate * 5));

  const dynamicFactor = 0.9 + 0.1 * Math.sin(frameCount / 45);
  return Math.min(1, naturalness * dynamicFactor);
}

function calculateDynamicFocus(features, frameCount) {
  const dynamicWeight = 0.5 + 0.3 * Math.sin(frameCount / 60);
  return (
    (features.browRaise * 0.5 +
      features.eyeWideness * 0.3 +
      (1 - features.jawTension) * 0.2) *
    dynamicWeight
  );
}

function calculateArticulation(features, frameCount) {
  const currentMouthActivity = Math.abs(
    features.mouthOpen - (1 - features.mouthPress)
  );
  mouthHistory.push(currentMouthActivity);
  if (mouthHistory.length > 15) mouthHistory.shift();

  let changeRate = 0;
  if (mouthHistory.length > 1) {
    changeRate =
      mouthHistory
        .slice(1)
        .map((val, i) => Math.abs(val - mouthHistory[i]))
        .reduce((a, b) => a + b, 0) /
      (mouthHistory.length - 1);
  }

  const baseScore =
    (1 - features.mouthPress) * (0.3 + 0.7 * Math.min(1, changeRate * 5));
  const dynamicFactor = 0.8 + 0.2 * Math.sin(frameCount / 75);
  return Math.min(1, baseScore * dynamicFactor);
}

function calculateEmotionalActivity(features, frameCount) {
  const expressionIntensity =
    Math.abs(features.mouthSmile - features.mouthFrown) * 0.6 +
    Math.abs(features.browRaise - features.browFrown) * 0.4;

  const dynamicBaseline = 0.3 + 0.2 * Math.sin(frameCount / 120);

  emotionHistory.push(expressionIntensity);
  if (emotionHistory.length > EMOTION_HISTORY_LENGTH) {
    emotionHistory.shift();
  }

  let changeRate = 0;
  if (emotionHistory.length > 1) {
    const changes = [];
    for (let i = 1; i < emotionHistory.length; i++) {
      changes.push(Math.abs(emotionHistory[i] - emotionHistory[i - 1]));
    }
    changeRate = changes.reduce((a, b) => a + b, 0) / changes.length;
  }

  return Math.min(
    1,
    (expressionIntensity * 0.6 + changeRate * 0.3 + dynamicBaseline * 0.1) * 1.2
  );
}

function calculateEmotionalPositivity(features) {
  const positiveSignals =
    features.mouthSmile * 0.5 +
    features.browRaise * 0.3 +
    (1 - features.browFrown) * 0.2;

  const negativeSignals =
    features.mouthFrown * 0.5 +
    features.browFrown * 0.3 +
    features.noseWrinkle * 0.2;

  const netPositivity = positiveSignals - negativeSignals * 0.7;
  return Math.min(1, Math.max(0, (netPositivity + 1) / 2));
}

function calculateShortTermVariance(history) {
  if (history.length < 5) return 0;
  const recent = history.slice(-5);
  const mean = recent.reduce((a, b) => a + b, 0) / recent.length;
  return recent.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / recent.length;
}

function getBlendshapeScore(blendshapes, shapeNames) {
  const relevantShapes = blendshapes.filter(
    (shape) => shapeNames.includes(shape.categoryName) && shape.score > 0.1
  );
  if (relevantShapes.length === 0) return 0;
  return (
    relevantShapes.reduce((sum, shape) => sum + shape.score, 0) /
    shapeNames.length
  );
}

function processFaceResults(results) {
  if (!results.faceBlendshapes || results.faceBlendshapes.length === 0) {
    faceExpression.value = "é¢éƒ¨: æ— ";
    return;
  }

  frameCount++;
  const blendshapes = results.faceBlendshapes[0].categories;

  // ç»˜åˆ¶é¢éƒ¨æ ‡è®°
  /*
  if (results.faceLandmarks && results.faceLandmarks.length > 0) {
    for (const landmarks of results.faceLandmarks) {
      drawingUtils.drawConnectors(
        landmarks,
        FaceLandmarker.FACE_LANDMARKS_TESSELATION,
        { color: "#C0C0C070", lineWidth: 1 }
      );
      drawingUtils.drawConnectors(
        landmarks,
        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,
        { color: "#FF3030", lineWidth: 2 }
      );
      drawingUtils.drawConnectors(
        landmarks,
        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,
        { color: "#30FF30", lineWidth: 2 }
      );
      drawingUtils.drawConnectors(
        landmarks,
        FaceLandmarker.FACE_LANDMARKS_LIPS,
        { color: "#E0E0E0", lineWidth: 2 }
      );
    }
  }
  */

  // åˆ†æåŠ¨æ€æŒ‡æ ‡
  const indicators = analyzeEmotionalIndicators(blendshapes, frameCount);

  // æ›´æ–°æ˜¾ç¤º - åŒ…å«æ–°æ—§æŒ‡æ ‡
  faceExpression.value =
    `çœ¼ç¥äº¤æµ: ${(indicators.eyeContact * 100).toFixed(0)}%\n` +
    `å¾®ç¬‘è‡ªç„¶åº¦: ${(indicators.smileNaturalness * 100).toFixed(0)}%\n` +
    `ä¸“æ³¨åº¦: ${(indicators.focusLevel * 100).toFixed(0)}%\n` +
    `è¡¨è¾¾æ¸…æ™°åº¦: ${(indicators.articulation * 100).toFixed(0)}%\n` +
    `æƒ…ç»ªæ´»è·ƒåº¦: ${(indicators.emotionalActivity * 100).toFixed(0)}%\n` +
    `æƒ…ç»ªç§¯æåº¦: ${(indicators.emotionalPositivity * 100).toFixed(0)}%`;

  // ç”Ÿæˆåé¦ˆ
  if (analysisReady.value) {
    feedbackList.value = generateEmotionalFeedback(indicators);
  }
}

function generateEmotionalFeedback(indicators) {
  const feedback = [];

  // åŸæœ‰æŒ‡æ ‡åé¦ˆ
  if (indicators.eyeContact < 0.5) {
    feedback.push("çœ¼ç¥äº¤æµä¸è¶³ï¼Œå»ºè®®æ¯3-5ç§’æœ‰æ„è¯†åœ°çœ‹å‘æ‘„åƒå¤´");
  } else if (indicators.eyeContact < 0.7) {
    feedback.push("çœ¼ç¥äº¤æµå°šå¯ï¼Œä½†å¯ä»¥æ›´é¢‘ç¹äº›");
  } else {
    feedback.push("çœ¼ç¥äº¤æµè‰¯å¥½ï¼Œä¿æŒè¿™ä¸ªçŠ¶æ€");
  }

  if (indicators.smileNaturalness < 0.4) {
    feedback.push("è¡¨æƒ…è¾ƒä¸¥è‚ƒï¼Œå°è¯•è‡ªç„¶å¾®ç¬‘");
  } else if (indicators.smileNaturalness < 0.6) {
    feedback.push("å¾®ç¬‘ç¨æ˜¾åƒµç¡¬ï¼Œè¯•ç€æ”¾æ¾é¢éƒ¨è‚Œè‚‰");
  } else if (indicators.smileNaturalness < 0.8) {
    feedback.push("å¾®ç¬‘è‡ªç„¶åº¦ä¸é”™ï¼Œç»§ç»­ä¿æŒ");
  } else {
    feedback.push("å¾®ç¬‘éå¸¸è‡ªç„¶äº²åˆ‡");
  }

  if (indicators.focusLevel < 0.5) {
    feedback.push("æ³¨æ„åŠ›ä¼¼ä¹ä¸å¤ªé›†ä¸­ï¼Œè¯•ç€æ›´æŠ•å…¥");
  } else if (indicators.focusLevel < 0.7) {
    feedback.push("ä¸“æ³¨åº¦å°šå¯ï¼Œä½†å¯ä»¥æ›´å¥½");
  } else {
    feedback.push("æ‚¨éå¸¸ä¸“æ³¨ï¼Œè¿™æ˜¯å¾ˆå¥½çš„è¡¨ç°");
  }

  if (indicators.articulation < 0.5) {
    feedback.push("å˜´éƒ¨åŠ¨ä½œè¾ƒå°ï¼Œå»ºè®®æ›´æ¸…æ™°åœ°å‘éŸ³");
  } else if (indicators.articulation < 0.7) {
    feedback.push("è¡¨è¾¾å°šæ¸…æ™°ï¼Œå¯ä»¥æ›´æ˜æ˜¾äº›");
  } else {
    feedback.push("è¡¨è¾¾éå¸¸æ¸…æ™°ï¼Œå¾ˆå¥½");
  }

  if (indicators.emotionalActivity > 0.7) {
    feedback.push("æƒ…ç»ªè¡¨è¾¾ä¸°å¯Œï¼Œä½†æ³¨æ„ä¿æŒç¨³å®š");
  } else if (indicators.emotionalActivity > 0.5) {
    feedback.push("æƒ…ç»ªè¡¨è¾¾é€‚åº¦ï¼Œæ•ˆæœä¸é”™");
  } else if (indicators.emotionalActivity > 0.3) {
    feedback.push("æƒ…ç»ªè¡¨è¾¾ç¨æ˜¾å¹³æ·¡ï¼Œå¯ä»¥æ›´ç”ŸåŠ¨äº›");
  } else {
    feedback.push("æƒ…ç»ªè¡¨è¾¾ä¸è¶³ï¼Œå»ºè®®å¢åŠ é¢éƒ¨è¡¨æƒ…");
  }

  if (indicators.emotionalPositivity > 0.7) {
    feedback.push("æƒ…ç»ªéå¸¸ç§¯æï¼Œç»™äººè‰¯å¥½å°è±¡");
  } else if (indicators.emotionalPositivity > 0.5) {
    feedback.push("æƒ…ç»ªåå‘ç§¯æï¼Œç»§ç»­ä¿æŒ");
  } else if (indicators.emotionalPositivity > 0.3) {
    feedback.push("æƒ…ç»ªä¸­æ€§ï¼Œå¯ä»¥å¢åŠ äº›ç§¯æè¡¨æƒ…");
  } else {
    feedback.push("æƒ…ç»ªåæ¶ˆæï¼Œå°è¯•å¾®ç¬‘æ”¹å–„å°è±¡");
  }

  return feedback;
}

function stopCamera() {
  console.log("[stopCamera] - Attempting to stop camera...");
  cameraOn.value = false;
  if (animationFrameId) {
    cancelAnimationFrame(animationFrameId);
    animationFrameId = null;
  }

  if (cameraView.value && cameraView.value.srcObject) {
    const stream = cameraView.value.srcObject;
    const tracks = stream.getTracks();
    tracks.forEach((track) => track.stop());
    cameraView.value.srcObject = null;
    cameraView.value.onloadeddata = null;
    console.log("[stopCamera] - Webcam stream stopped and detached.");
  }
  if (overlayCtx && overlay.value) {
    overlayCtx.clearRect(0, 0, overlay.value.width, overlay.value.height);
  }
  gesture.value = "æ— ";
  faceExpression.value = "é¢éƒ¨: æ— ";
  lastVideoTime = -1;
  analysisReady.value = false;
}

function analyze() {
  if (cameraOn.value) {
    analysisReady.value = true;
  } else {
    feedbackList.value = ["è¯·å…ˆå¼€å¯æ‘„åƒå¤´å†è¿›è¡ŒAIåˆ†æã€‚"];
    analysisReady.value = false;
  }
}

onMounted(async () => {
  const SpeechRecognition =
    window.SpeechRecognition || window.webkitSpeechRecognition;
  if (SpeechRecognition) {
    recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = false;
    recognition.lang = "zh-CN";

    // recognition.onresult = (event) => {
    //   let interim = "";
    //   for (let i = event.resultIndex; i < event.results.length; i++) {
    //     const text = event.results[i][0].transcript;
    //     if (event.results[i].isFinal) {
    //       transcript.value += text;
    //       evaluateWithDeepSeek(transcript.value)
    //     } else {
    //       interim += text;
    //     }
    //   }
    //   interimTranscript.value = interim
    // };
    // recognition.onresult = (event) => {
    //   // åªå¤„ç† isFinal=trueï¼Œå³æœ€ç»ˆç»“æœ
    //   for (let i = event.resultIndex; i < event.results.length; i++) {
    //     if (event.results[i].isFinal) {
    //       transcript.value += event.results[i][0].transcript
    //       evaluateWithDeepSeek(transcript.value) // åœåæ‰è°ƒç”¨ DeepSeek
    //     }
    //   }
    // }
    recognition.onresult = (event) => {
      let interim = "";
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const txt = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          transcript.value += txt;
        } else {
          interim += txt;
        }
      }
      interimTranscript.value = interim;
    };

    recognition.onerror = (e) => {
      console.error("è¯­éŸ³è¯†åˆ«å‡ºé”™ï¼š", e);
    };

    recognition.onend = () => {
      isRecognizing.value = false;
    };
  } else {
    console.warn("æµè§ˆå™¨ä¸æ”¯æŒ SpeechRecognition");
  }
  console.log("Vue component MOUNTED. Initializing models...");
  await createGestureRecognizer();
  await createFaceLandmarker();
  showRandomQuestion();
});

onUnmounted(() => {
  console.log("Vue component UNMOUNTED. Cleaning up...");
  stopCamera();
  if (gestureRecognizer) {
    gestureRecognizer.close();
    gestureRecognizer = null;
  }
  if (faceLandmarker) {
    faceLandmarker.close();
    faceLandmarker = null;
  }
  if (recognition && isRecognizing.value) {
    recognition.stop();
  }
});

function startRecognition() {
  // if (recognition && !isRecognizing.value) {
  //   transcript.value = "";
  //   recognition.start();
  //   isRecognizing.value = true;
  // }
  if (isRecognizing.value) {
    recognition.stop();
    isRecognizing.value = false;
    evaluateWithDeepSeek(transcript.value);
  } else {
    transcript.value = "";
    evaluation.value = "";
    recognition.start();
    isRecognizing.value = true;
  }
}

// function stopRecognition() {
//   if (recognition && isRecognizing.value) {
//     recognition.stop();
//   }
// }

const evaluation = ref("");
const loadingEval = ref(false);

// sk-0f909e0c234c4fd3a582966371e29f63
async function evaluateWithDeepSeek(text) {
  console.log(text);
  if (!text) {
    evaluation.value = "";
    return;
  }
  loadingEval.value = true;
  const openai = new OpenAI({
    baseURL: "https://api.deepseek.com",
    apiKey: "sk-0f909e0c234c4fd3a582966371e29f63",
    dangerouslyAllowBrowser: true,
  });
  try {
    const completion = await openai.chat.completions.create({
      messages: [
        {
          role: "system",
          content:
            "ä½ æ˜¯ä¸€ä½AIé¢è¯•å®˜ï¼Œéœ€è¦å¸®åŠ©ç”¨æˆ·ä¼˜åŒ–é¢è¯•è¡¨ç°ã€‚å¯¹äºç»™å‡ºçš„é—®é¢˜å’Œç”¨æˆ·çš„ä½œç­”ï¼Œç»™å‡ºä¸“ä¸šï¼Œæœ‰ä»·å€¼ï¼Œå…·æœ‰å¸®åŠ©æ€§ï¼Œæ€åº¦å‹å¥½çš„å›ç­”ã€‚å†…å®¹ä¸Šå›ç­”é¡ºåºä¸º[æ€»ä½“è¯„ä»·][ä¼˜ç‚¹][æ”¹è¿›å»ºè®®]ã€‚å½¢å¼ä¸Šä»¥æ¸…æ™°ç¾è§‚ä¸”å¯ä»¥è¢«jsçš„markedåŒ…ç›´æ¥æ¸²æŸ“çš„æ ¼å¼è¾“å‡ºã€‚",
        },
        {
          role: "user",
          content: "å¯¹äºé—®é¢˜ï¼š" + question + "\nç”¨æˆ·çš„å›ç­”æ˜¯: " + text,
        },
      ],
      model: "deepseek-chat",
    });
    evaluation.value = completion.choices[0].message.content;
    feedbackList.value = [evaluation.value];
  } catch (err) {
    console.error("DeepSeek è¯„ä»·å¤±è´¥ï¼š", err);
    evaluation.value = "è¯„ä»·æœåŠ¡ä¸å¯ç”¨";
  } finally {
    loadingEval.value = false;
  }
}

function renderMarkdown(text) {
  const norm = text.replace(/```[a-zA-Z]*\n/, "").replace(/```/, "");
  console.log(norm);
  return marked.parse(norm);
}
</script>

<style scoped>
@import "../assets/interview.css";
</style>
